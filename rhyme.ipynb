{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83185beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install scikit-learn\n",
    "!pip install torch==2.6.0\n",
    "!pip install torchtext==0.17.2\n",
    "!pip install matplotlib\n",
    "!pip install pandas\n",
    "!pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617c9d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import string\n",
    "import time\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "%capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44220c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "song= \"\"\"We are no strangers to love\n",
    "You know the rules and so do I\n",
    "A full commitments what Im thinking of\n",
    "You wouldnt get this from any other guy\n",
    "I just wanna tell you how Im feeling\n",
    "Gotta make you understand\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Weve known each other for so long\n",
    "Your hearts been aching but youre too shy to say it\n",
    "Inside we both know whats been going on\n",
    "We know the game and were gonna play it\n",
    "And if you ask me how Im feeling\n",
    "Dont tell me youre too blind to see\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Weve known each other for so long\n",
    "Your hearts been aching but youre too shy to say it\n",
    "Inside we both know whats been going on\n",
    "We know the game and were gonna play it\n",
    "I just wanna tell you how Im feeling\n",
    "Gotta make you understand\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60df805",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens=tokenizer(song)\n",
    "\n",
    "def preprocess_string(s):\n",
    "    \"\"\"\n",
    "    Preprocesses a given string by performing the following steps:\n",
    "    \n",
    "    1. Removes all non-word characters (excluding letters and numbers).\n",
    "    2. Removes all whitespace characters.\n",
    "    3. Removes all numeric digits.\n",
    "\n",
    "    Parameters:\n",
    "    s (str): The input string to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "    str: The processed string with only alphabetic characters, no spaces, and no digits.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove all non-word characters (everything except letters and numbers)\n",
    "    # \\w matches any word character (letters, numbers, and underscores)\n",
    "    # \\s matches any whitespace characters\n",
    "    # ^ inside [] negates the selection, so [^\\w\\s] matches anything that's NOT a word character or whitespace.\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "\n",
    "    # Remove all whitespace characters (spaces, tabs, newlines)\n",
    "    # \\s+ matches one or more whitespace characters.\n",
    "    s = re.sub(r\"\\s+\", '', s)\n",
    "\n",
    "    # Remove all digits (0-9)\n",
    "    # \\d matches any digit character.\n",
    "    s = re.sub(r\"\\d\", '', s)\n",
    "\n",
    "    return s\n",
    "\n",
    "def preprocess(words):\n",
    "    \"\"\"\n",
    "    Preprocesses a given text by tokenizing it, cleaning individual words, and \n",
    "    converting them to lowercase while removing empty or punctuation tokens.\n",
    "\n",
    "    Steps:\n",
    "    1. Tokenization: Splits the input text into individual word tokens.\n",
    "    2. Cleaning: Applies `preprocess_string()` to remove non-word characters, \n",
    "       spaces, and digits from each token.\n",
    "    3. Normalization: Converts all tokens to lowercase.\n",
    "    4. Filtering: Removes empty strings and punctuation tokens.\n",
    "\n",
    "    Parameters:\n",
    "    words (str): The input text to be tokenized and preprocessed.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of cleaned, lowercase tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the input text into words\n",
    "    tokens = word_tokenize(words)\n",
    "\n",
    "    # Apply preprocessing to each token (removes unwanted characters)\n",
    "    tokens = [preprocess_string(w) for w in tokens]\n",
    "\n",
    "    # Convert tokens to lowercase and remove empty strings or punctuation\n",
    "    return [w.lower() for w in tokens if len(w) != 0 and w not in string.punctuation]\n",
    "\n",
    "# Example usage:\n",
    "tokens = preprocess(song)  # Preprocess the text in 'song'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7303a52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizetext(song):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text (song) and builds a vocabulary from the tokens.\n",
    "\n",
    "    Steps:\n",
    "    1. Tokenization: The function splits the input text into words and applies \n",
    "       a tokenizer function to each word.\n",
    "    2. Vocabulary Building: Constructs a vocabulary from the tokenized words,\n",
    "       including a special \"<unk>\" token to handle out-of-vocabulary words.\n",
    "    3. Default Indexing: Sets the default index for unknown words, ensuring \n",
    "       that any unseen tokens are mapped to \"<unk>\".\n",
    "\n",
    "    Parameters:\n",
    "    song (str): The input text (song lyrics) to be tokenized and processed.\n",
    "\n",
    "    Returns:\n",
    "    vocab (Vocab): A vocabulary object mapping tokens to their corresponding indices.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the text\n",
    "    # Split the input text into words and apply the tokenizer function to each word.\n",
    "    # The 'map' function ensures that each word is tokenized properly.\n",
    "    tokenized_song = map(tokenizer, song.split())\n",
    "\n",
    "    # Build vocabulary from tokenized text\n",
    "    # The function `build_vocab_from_iterator` constructs a vocabulary by iterating \n",
    "    # over the tokenized words. The special token \"<unk>\" is added to handle words \n",
    "    # that are not present in the vocabulary.\n",
    "    vocab = build_vocab_from_iterator(tokenized_song, specials=[\"<unk>\"])\n",
    "\n",
    "    # Set the default index for unknown words\n",
    "    # The default index is set to the index of \"<unk>\" so that any word not found \n",
    "    # in the vocabulary is mapped to this token, preventing errors during lookup.\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8177036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=tokenizetext(song)\n",
    "vocab(tokens[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ed7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0455dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "text_pipeline(song)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be1e332",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_token = vocab.get_itos()\n",
    "index_to_token[58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe4466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genembedding(vocab):\n",
    "    \"\"\"\n",
    "    Generates an embedding layer for the given vocabulary.\n",
    "\n",
    "    The embedding layer transforms words into dense vector representations, \n",
    "    allowing the model to learn semantic relationships between words.\n",
    "\n",
    "    Parameters:\n",
    "    vocab (Vocab): The vocabulary object containing unique words and their indices.\n",
    "\n",
    "    Returns:\n",
    "    nn.Embedding: A PyTorch embedding layer with a specified embedding dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the embedding dimension (size of word vectors)\n",
    "    embedding_dim = 20  # Each word will be represented as a 20-dimensional vector\n",
    "\n",
    "    # Get the vocabulary size (number of unique words in the vocabulary)\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # Create an embedding layer\n",
    "    # The nn.Embedding module maps word indices to dense vector representations.\n",
    "    # It takes vocab_size as the number of words and embedding_dim as the vector size.\n",
    "    embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf127fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=genembedding(vocab)\n",
    "for n in range(2): \n",
    "    embedding=embeddings(torch.tensor(n))\n",
    "    print(\"word\",index_to_token[n])\n",
    "    print(\"index\",n)\n",
    "    print( \"embedding\", embedding)\n",
    "    print(\"embedding shape\", embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37647484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the context size for generating n-grams\n",
    "CONTEXT_SIZE = 2  # The number of previous words used to predict the next word\n",
    "\n",
    "def genngrams(tokens):\n",
    "    \"\"\"\n",
    "    Generates n-grams from a list of tokens, where each n-gram consists of a \n",
    "    context (previous words) and a target (next word).\n",
    "\n",
    "    The function constructs a list of tuples where:\n",
    "    - The first element is a list of `CONTEXT_SIZE` previous words.\n",
    "    - The second element is the target word that follows the context.\n",
    "\n",
    "    Parameters:\n",
    "    tokens (list): A list of preprocessed word tokens.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of tuples representing n-grams.\n",
    "          Each tuple contains (context_words, target_word).\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate n-grams\n",
    "    # Iterate through the tokens starting from index CONTEXT_SIZE to the end\n",
    "    # For each token at position 'i', extract the previous CONTEXT_SIZE words as context\n",
    "    ngrams = [\n",
    "        (\n",
    "            [tokens[i - j - 1] for j in range(CONTEXT_SIZE)],  # Context words (previous words)\n",
    "            tokens[i]  # Target word (the word to predict)\n",
    "        )\n",
    "        for i in range(CONTEXT_SIZE, len(tokens))\n",
    "    ]\n",
    "\n",
    "    return ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfda25a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams=genngrams(tokens)\n",
    "context, target=ngrams[0]\n",
    "print(\"context\",context,\"target\",target)\n",
    "print(\"context index\",vocab(context),\"target index\",vocab([target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c95926",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=20\n",
    "linear = nn.Linear(embedding_dim*CONTEXT_SIZE,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4ed265",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=genembedding(vocab)\n",
    "my_embeddings=embeddings(torch.tensor(vocab(context)))\n",
    "my_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cff2102",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_embeddings=my_embeddings.reshape(1,-1)\n",
    "my_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85b2967",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear(my_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1004d635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch func w data loader\n",
    "from torch.utils.data import DataLoader  # Importing DataLoader for batch processing\n",
    "import torch  # Importing PyTorch\n",
    "\n",
    "# Set the device to GPU if available; otherwise, use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define hyperparameters\n",
    "CONTEXT_SIZE = 3   # Number of previous words used as context for prediction\n",
    "BATCH_SIZE = 10    # Number of samples per training batch\n",
    "EMBEDDING_DIM = 10 # Dimension of word embeddings\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    Processes a batch of text data into input (context) and output (target) tensors\n",
    "    for training a language model.\n",
    "\n",
    "    The function extracts:\n",
    "    - `context`: A list of word indices representing the context words for each target word.\n",
    "    - `target`: A list of word indices representing the target word to predict.\n",
    "\n",
    "    Parameters:\n",
    "    batch (list): A list of tokenized words (strings).\n",
    "\n",
    "    Returns:\n",
    "    tuple: Two PyTorch tensors: (context_tensor, target_tensor)\n",
    "           - context_tensor: Tensor of shape (batch_size - CONTEXT_SIZE, CONTEXT_SIZE),\n",
    "             containing the word indices of context words.\n",
    "           - target_tensor: Tensor of shape (batch_size - CONTEXT_SIZE,),\n",
    "             containing the word indices of target words.\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = len(batch)  # Get the size of the batch\n",
    "    context, target = [], [] # Initialize lists for context and target words\n",
    "\n",
    "    # Loop through the batch, ensuring enough previous words exist for context\n",
    "    for i in range(CONTEXT_SIZE, batch_size):\n",
    "        # Convert the target word to its index using the vocabulary\n",
    "        target.append(vocab([batch[i]]))\n",
    "\n",
    "        # Convert the previous CONTEXT_SIZE words to indices using the vocabulary\n",
    "        context.append(vocab([batch[i - j - 1] for j in range(CONTEXT_SIZE)]))\n",
    "\n",
    "    # Convert lists to PyTorch tensors and move them to the appropriate device (CPU/GPU)\n",
    "    return torch.tensor(context).to(device), torch.tensor(target).to(device).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f04772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding\n",
    "Padding=BATCH_SIZE-len(tokens)%BATCH_SIZE\n",
    "tokens_pad=tokens+tokens[0:Padding]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f2b45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "     tokens_pad, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bb4f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network-based n-gram language model that predicts the next word \n",
    "    given a sequence of context words.\n",
    "\n",
    "    This model consists of:\n",
    "    - An embedding layer that converts word indices into dense vector representations.\n",
    "    - A fully connected hidden layer with ReLU activation.\n",
    "    - An output layer that predicts the probability distribution over the vocabulary.\n",
    "\n",
    "    Parameters:\n",
    "    vocab_size (int): The number of unique words in the vocabulary.\n",
    "    embedding_dim (int): The size of the word embeddings (vector representation of words).\n",
    "    context_size (int): The number of previous words used as context to predict the next word.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "\n",
    "        # Store context size and embedding dimension\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Embedding layer: Maps word indices to dense vectors\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Fully connected hidden layer: Maps the concatenated embeddings to a 128-dimensional space\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "\n",
    "        # Output layer: Maps the hidden layer output to vocabulary size (probability distribution over words)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        inputs (Tensor): A tensor of shape (batch_size, context_size) containing word indices.\n",
    "\n",
    "        Returns:\n",
    "        Tensor: A tensor of shape (batch_size, vocab_size) representing predicted probabilities for the next word.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert input word indices into dense vectors using the embedding layer\n",
    "        embeds = self.embeddings(inputs)  # Shape: (batch_size, context_size, embedding_dim)\n",
    "\n",
    "        # Reshape the embeddings into a single vector per input sample\n",
    "        embeds = torch.reshape(embeds, (-1, self.context_size * self.embedding_dim))  \n",
    "        # New shape: (batch_size, context_size * embedding_dim)\n",
    "\n",
    "        # Apply first fully connected layer with ReLU activation\n",
    "        out = F.relu(self.linear1(embeds))  # Shape: (batch_size, 128)\n",
    "\n",
    "        # Apply second fully connected layer to generate vocabulary-size logits\n",
    "        out = self.linear2(out)  # Shape: (batch_size, vocab_size)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d367079",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35168c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get samples from data loader\n",
    "context, target=next(iter(dataloader))\n",
    "print(context, target)\n",
    "out=model(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8f1edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index w highest prob\n",
    "predicted_index =torch.argmax(out,1)\n",
    "print(predicted_index)\n",
    "\n",
    "#corresponding token \n",
    "[index_to_token[i.item()] for i in  predicted_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a4f6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_song(model, my_song, number_of_words=100):\n",
    "    \"\"\"\n",
    "    Generates text using a trained n-gram language model.\n",
    "\n",
    "    Given an initial text (`my_song`), the function generates additional words by \n",
    "    predicting the next word iteratively based on the trained model.\n",
    "\n",
    "    Parameters:\n",
    "    model (nn.Module): The trained n-gram language model.\n",
    "    my_song (str): The initial seed text to start generating words.\n",
    "    number_of_words (int): The number of words to generate (default: 100).\n",
    "\n",
    "    Returns:\n",
    "    str: The generated song lyrics as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the mapping from index to word for decoding predictions\n",
    "    index_to_token = vocab.get_itos()\n",
    "\n",
    "    # Loop to generate the desired number of words\n",
    "    for i in range(number_of_words):\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation for inference\n",
    "            \n",
    "            # Prepare the input context by extracting the last CONTEXT_SIZE words from tokens\n",
    "            context = torch.tensor(\n",
    "                vocab([tokens[i - j - 1] for j in range(CONTEXT_SIZE)])\n",
    "            ).to(device)  # Move to CPU/GPU as required\n",
    "            \n",
    "            # Predict the next word by selecting the word with the highest probability\n",
    "            word_idx = torch.argmax(model(context))  # Get index of the most likely next word\n",
    "            \n",
    "            # Append the predicted word to the generated text\n",
    "            my_song += \" \" + index_to_token[word_idx.detach().item()]\n",
    "\n",
    "    return my_song  # Return the generated lyrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b47b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickrandomline(song):\n",
    "    \"\"\"\n",
    "    Selects a random line from the given song text.\n",
    "\n",
    "    This function splits the song into separate lines and randomly picks one of them.\n",
    "\n",
    "    Parameters:\n",
    "    song (str): The song lyrics as a multi-line string.\n",
    "\n",
    "    Returns:\n",
    "    str: A randomly selected line from the song.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the song into individual lines\n",
    "    lines = song.split(\"\\n\")  \n",
    "    \n",
    "    # Randomly select a line and remove leading/trailing whitespace\n",
    "    selected_line = random.choice(lines).strip()\n",
    "    \n",
    "    return selected_line  # Return the randomly selected line\n",
    "\n",
    "# Example usage:\n",
    "selected_line = pickrandomline(song)  # Pick a random line from the song\n",
    "\n",
    "# Generate a new song starting with the selected line\n",
    "generated_song = write_song(model, selected_line)\n",
    "\n",
    "# Print the generated lyrics\n",
    "print(generated_song)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01f2e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9da833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model,song,number_of_epochs=100, show=10):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataloader (DataLoader): DataLoader containing training data.\n",
    "        model (nn.Module): Neural network model to be trained.\n",
    "        number_of_epochs (int, optional): Number of epochs for training. Default is 100.\n",
    "        show (int, optional): Interval for displaying progress. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "        list: List containing loss values for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    MY_LOSS = []  # List to store loss values for each epoch\n",
    "\n",
    "    # Iterate over the specified number of epochs\n",
    "    for epoch in tqdm(range(number_of_epochs)):\n",
    "        total_loss = 0  # Initialize total loss for the current epoch\n",
    "        my_song = \"\"    # Initialize a string to store the generated song\n",
    "\n",
    "        # Iterate over batches in the dataloader\n",
    "        for context, target in dataloader:\n",
    "            model.zero_grad()          # Zero the gradients to avoid accumulation\n",
    "            predicted = model(context)  # Forward pass through the model to get predictions\n",
    "            loss = criterion(predicted, target.reshape(-1))  # Calculate the loss\n",
    "            total_loss += loss.item()   # Accumulate the loss\n",
    "\n",
    "            loss.backward()    # Backpropagation to compute gradients\n",
    "            optimizer.step()   # Update model parameters using the optimizer\n",
    "\n",
    "        # Display progress and generate song at specified intervals\n",
    "        if epoch % show == 0:\n",
    "            selected_line=pickrandomline(song)\n",
    "            my_song += write_song(model, selected_line)    # Generate song using the model\n",
    "\n",
    "            print(\"Generated Song:\")\n",
    "            print(\"\\n\")\n",
    "            print(my_song)\n",
    "\n",
    "        MY_LOSS.append(total_loss/len(dataloader))  # Append the total loss for the epoch to MY_LOSS list\n",
    "\n",
    "    return MY_LOSS  # Return the list of  mean loss values for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e68424d",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "my_loss_list=[]\n",
    "\n",
    "#initialize n gram model context size 2\n",
    "\n",
    "# Define the context size for the n-gram model\n",
    "CONTEXT_SIZE = 2\n",
    "\n",
    "# Create an instance of the NGramLanguageModeler class with specified parameters\n",
    "model_2 = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE).to(device)\n",
    "\n",
    "# Define the optimizer for training the model, using stochastic gradient descent (SGD)\n",
    "optimizer = optim.SGD(model_2.parameters(), lr=0.01)\n",
    "\n",
    "# Set up a learning rate scheduler using StepLR to adjust the learning rate during training\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1.0, gamma=0.1)\n",
    "\n",
    "\n",
    "#train model\n",
    "my_loss=train(dataloader,model_2,song)\n",
    "\n",
    "#save model\n",
    "save_path = '2gram.pth'\n",
    "torch.save(model_2.state_dict(), save_path)\n",
    "my_loss_list.append(my_loss)\n",
    "\n",
    "#plot word embedding in 2D\n",
    "X = model_2.embeddings.weight.cpu().detach().numpy()\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_2d = tsne.fit_transform(X)\n",
    "\n",
    "labels = []\n",
    "\n",
    "for j in range(len(X_2d)):\n",
    "    if j < 20:\n",
    "        plt.scatter(X_2d[j, 0], X_2d[j, 1], label=index_to_token[j])\n",
    "        labels.append(index_to_token[j])\n",
    "        # Add words as annotations\n",
    "        plt.annotate(index_to_token[j],\n",
    "                     (X_2d[j, 0], X_2d[j, 1]),\n",
    "                     textcoords=\"offset points\",\n",
    "                     xytext=(0, 10),\n",
    "                     ha='center')\n",
    "    else:\n",
    "        plt.scatter(X_2d[j, 0], X_2d[j, 1])\n",
    "\n",
    "plt.legend(labels, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb32c93",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "#repeat for context size 4\n",
    "CONTEXT_SIZE=4\n",
    "model_4 = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE).to(device)\n",
    "optimizer = optim.SGD(model_4.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "my_loss=train(dataloader,model_4,song)\n",
    "\n",
    "save_path = '4gram.pth'\n",
    "torch.save(model_4.state_dict(), save_path)\n",
    "\n",
    "my_loss_list.append(my_loss)\n",
    "\n",
    "X = model_4.embeddings.weight.cpu().detach().numpy()\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_2d = tsne.fit_transform(X)\n",
    "\n",
    "labels = []\n",
    "\n",
    "for j in range(len(X_2d)):\n",
    "    if j < 20:\n",
    "        plt.scatter(X_2d[j, 0], X_2d[j, 1], label=index_to_token[j])\n",
    "        labels.append(index_to_token[j])\n",
    "        # Add words as annotations\n",
    "        plt.annotate(index_to_token[j],\n",
    "                     (X_2d[j, 0], X_2d[j, 1]),\n",
    "                     textcoords=\"offset points\",\n",
    "                     xytext=(0, 10),\n",
    "                     ha='center')\n",
    "    else:\n",
    "        plt.scatter(X_2d[j, 0], X_2d[j, 1])\n",
    "\n",
    "plt.legend(labels, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d9b256",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "#repeat for 8\n",
    "CONTEXT_SIZE=8\n",
    "model_8 = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE).to(device)\n",
    "optimizer = optim.SGD(model_8.parameters(), lr=0.01)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "my_loss=train(dataloader,model_8,song)\n",
    "\n",
    "save_path = '8gram.pth'\n",
    "torch.save(model_8.state_dict(), save_path)\n",
    "\n",
    "my_loss_list.append(my_loss)\n",
    "\n",
    "X = model_8.embeddings.weight.cpu().detach().numpy()\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_2d = tsne.fit_transform(X)\n",
    "\n",
    "labels = []\n",
    "\n",
    "for j in range(len(X_2d)):\n",
    "    if j < 20:\n",
    "        plt.scatter(X_2d[j, 0], X_2d[j, 1], label=index_to_token[j])\n",
    "        labels.append(index_to_token[j])\n",
    "        # Add words as annotations\n",
    "        plt.annotate(index_to_token[j],\n",
    "                     (X_2d[j, 0], X_2d[j, 1]),\n",
    "                     textcoords=\"offset points\",\n",
    "                     xytext=(0, 10),\n",
    "                     ha='center')\n",
    "    else:\n",
    "        plt.scatter(X_2d[j, 0], X_2d[j, 1])\n",
    "\n",
    "plt.legend(labels, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab23e45",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "for (my_loss, model_name)in zip(my_loss_list,[\"2-gram\",\"4-gram\",\"8-gram\"]):\n",
    "    plt.plot(my_loss,label=\"Cross-entropy Loss - {}\".format(model_name))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e28d30",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "#perplexity - A lower perplexity value indicates that the model is more confident and accurate in predicting the data. \n",
    "#Conversely, a higher perplexity suggests that the model is less certain and less accurate in its predictions.\n",
    "\n",
    "for (my_loss, model_name)in zip(my_loss_list,[\"2-gram\",\"4-gram\",\"8-gram\"]):\n",
    "    # Calculate perplexity using the loss\n",
    "    perplexity = np.exp(my_loss)\n",
    "    plt.plot(perplexity,label=\"Perplexity - {}\".format(model_name))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efbe9e6",
   "metadata": {},
   "source": [
    "NURSERY RHYMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9eb2fb",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "nursery_rhymes = \"\"\"\n",
    "Little Miss Muffet\n",
    "Sat on a tuffet,\n",
    "Eating her curds and whey;\n",
    "Along came a spider\n",
    "Who sat down beside her\n",
    "And frightened Miss Muffet away.\n",
    "\n",
    "Twinkle, twinkle, little star,\n",
    "How I wonder what you are!\n",
    "Up above the world so high,\n",
    "Like a diamond in the sky.\n",
    "\n",
    "Baa, baa, black sheep,\n",
    "Have you any wool?\n",
    "Yes sir, yes sir,\n",
    "Three bags full.\n",
    "\n",
    "Jack and Jill went up the hill\n",
    "To fetch a pail of water.\n",
    "Jack fell down and broke his crown,\n",
    "And Jill came tumbling after.\n",
    "\n",
    "Hickory dickory dock,\n",
    "The mouse ran up the clock.\n",
    "The clock struck one,\n",
    "The mouse ran down,\n",
    "Hickory dickory dock.\n",
    "\n",
    "Humpty Dumpty sat on a wall,\n",
    "Humpty Dumpty had a great fall.\n",
    "All the king's horses and all the king's men\n",
    "Couldn't put Humpty together again.\n",
    "\n",
    "Mary had a little lamb,\n",
    "Its fleece was white as snow;\n",
    "And everywhere that Mary went,\n",
    "The lamb was sure to go.\n",
    "\n",
    "Old MacDonald had a farm,\n",
    "E-I-E-I-O,\n",
    "And on his farm he had a cow,\n",
    "E-I-E-I-O.\n",
    "\n",
    "Itsy Bitsy Spider climbed up the waterspout.\n",
    "Down came the rain and washed the spider out.\n",
    "Out came the sun and dried up all the rain,\n",
    "And the Itsy Bitsy Spider climbed up the spout again.\n",
    "\n",
    "The wheels on the bus go round and round,\n",
    "Round and round,\n",
    "Round and round.\n",
    "The wheels on the bus go round and round,\n",
    "All through the town.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7ae9e6",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "#tokenize & n-grams\n",
    "# Define the value of N for N-grams (context size)\n",
    "N = 2  \n",
    "\n",
    "# Preprocess the text (e.g., nursery rhymes) to tokenize and clean it\n",
    "tokens = preprocess(nursery_rhymes)  # Use the preprocess function to tokenize the text\n",
    "\n",
    "# Generate N-grams using the `genngrams` function\n",
    "ngrams = genngrams(tokens)  \n",
    "\n",
    "# Extract the first context-target pair from the list of N-grams\n",
    "context, target = ngrams[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ec0491",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "#words -> embeddings + pass thru linear layer\n",
    "\n",
    "# Tokenize text and create vocabulary\n",
    "vocab = tokenizetext(nursery_rhymes)  \n",
    "\n",
    "\n",
    "# Define embedding dimensions and create layers\n",
    "embedding_dim = 20  # Each word will be represented as a 20-dimensional vector\n",
    "linear = nn.Linear(embedding_dim * CONTEXT_SIZE, 128)  \n",
    "\n",
    "\n",
    "# Generate embeddings using the custom embedding function\n",
    "embeddings = genembedding(vocab)  \n",
    "\n",
    "\n",
    "# Convert context words into embeddings\n",
    "my_embeddings = embeddings(torch.tensor(vocab(context)))  \n",
    "\n",
    "\n",
    "# Reshape embeddings to match the input shape required by the linear layer\n",
    "my_embeddings = my_embeddings.reshape(1, -1)  \n",
    "\n",
    "\n",
    "# Pass embeddings through the linear layer\n",
    "output = linear(my_embeddings)  \n",
    "\n",
    "\n",
    "# Print output shape for verification\n",
    "print(\"Output shape:\", output.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48f90ac",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "#batch processing + padding\n",
    "# Define constants for training\n",
    "CONTEXT_SIZE = 3  # The number of words in the context window\n",
    "BATCH_SIZE = 10   # The number of samples per batch\n",
    "EMBEDDING_DIM = 10  # The dimension of the word embeddings\n",
    "\n",
    "# Compute padding to ensure the number of tokens is evenly divisible by the batch size\n",
    "Padding = BATCH_SIZE - len(tokens) % BATCH_SIZE  \n",
    "\n",
    "\n",
    "tokens_pad = tokens + tokens[0:Padding]  \n",
    "\n",
    "\n",
    "# Define device (CPU/GPU) for computation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "dataloader = DataLoader(\n",
    "    tokens_pad, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch\n",
    ")  \n",
    "\n",
    "\n",
    "# Print length of total tokens after padding\n",
    "print(f\"Total tokens (after padding): {len(tokens_pad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be377b3d",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "#train ngram model\n",
    "# Define context size for N-gram modeling\n",
    "CONTEXT_SIZE = 2  # Number of previous words used as context for predicting the next word\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "\n",
    "# Initialize the N-gram language model\n",
    "model3 = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE).to(device)  \n",
    "\n",
    "\n",
    "# Define the optimizer (Stochastic Gradient Descent)\n",
    "optimizer = optim.SGD(model3.parameters(), lr=0.01)  \n",
    "\n",
    "# Implement Learning Rate Scheduling\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)  \n",
    "\n",
    "# Train the model and track loss history\n",
    "loss_history = train(dataloader, model3, nursery_rhymes)  \n",
    "\n",
    "\n",
    "# Output expected behavior\n",
    "print(f\"Training started with {len(vocab)} words in vocabulary.\")\n",
    "print(f\"Optimizer: {optimizer}\")\n",
    "print(f\"Learning rate scheduler: {scheduler}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0d4363",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "#generate new rhyme\n",
    "# Select a random line from the nursery rhymes dataset\n",
    "selected_line = pickrandomline(nursery_rhymes)  \n",
    "\n",
    "\n",
    "# Generate a new rhyme using the trained language model\n",
    "generated_rhyme = write_song(model3, selected_line)  \n",
    "\n",
    "\n",
    "# Print the generated rhyme\n",
    "print(generated_rhyme) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb3049f",
   "metadata": {},
   "source": [
    "NLTK Histogram N-Gram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e000b81",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install scikit-learn\n",
    "!pip install torch==2.6.0\n",
    "!pip install torchtext==0.17.2\n",
    "!pip install matplotlib\n",
    "!pip install pandas\n",
    "!pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a657f072",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import string\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "%capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d723d1a4",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_string(s):\n",
    "    # Remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    # Replace all runs of whitespaces with no space\n",
    "    s = re.sub(r\"\\s+\", '', s)\n",
    "    # replace digits with no space\n",
    "    s = re.sub(r\"\\d\", '', s)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed98edd4",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "song= \"\"\"We are no strangers to love\n",
    "You know the rules and so do I\n",
    "A full commitments what Im thinking of\n",
    "You wouldnt get this from any other guy\n",
    "I just wanna tell you how Im feeling\n",
    "Gotta make you understand\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Weve known each other for so long\n",
    "Your hearts been aching but youre too shy to say it\n",
    "Inside we both know whats been going on\n",
    "We know the game and were gonna play it\n",
    "And if you ask me how Im feeling\n",
    "Dont tell me youre too blind to see\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Weve known each other for so long\n",
    "Your hearts been aching but youre too shy to say it\n",
    "Inside we both know whats been going on\n",
    "We know the game and were gonna play it\n",
    "I just wanna tell you how Im feeling\n",
    "Gotta make you understand\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d19ee4a",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "#tikenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "def preprocess(words):\n",
    "    tokens=word_tokenize(words)\n",
    "    tokens=[preprocess_string(w)   for w in tokens]\n",
    "    return [w.lower()  for w in tokens if len(w)!=0 or not(w in string.punctuation) ]\n",
    "\n",
    "tokens=preprocess(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e752059",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "# Create a frequency distribution of words\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "fdist\n",
    "\n",
    "plt.bar(list(fdist.keys())[0:10],list(fdist.values())[0:10])\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e8e5c",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "#total count of each word \n",
    "C=sum(fdist.values())\n",
    "\n",
    "\n",
    "#token to set\n",
    "vocabulary=set(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2a4375",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "#bigram\n",
    "bigrams = nltk.bigrams(tokens)\n",
    "my_bigrams=list(nltk.bigrams(tokens))\n",
    "freq_bigrams  = nltk.FreqDist(nltk.bigrams(tokens))\n",
    "\n",
    "#conditional distribution\n",
    "word=\"strangers\"\n",
    "vocab_probabilities={}\n",
    "for next_word in vocabulary:\n",
    "    vocab_probabilities[next_word]=freq_bigrams[(word,next_word)]/fdist[word]\n",
    "\n",
    "vocab_probabilities=sorted(vocab_probabilities.items(), key=lambda x:x[1],reverse=True)\n",
    "\n",
    "\n",
    "#words more likely to occur\n",
    "vocab_probabilities[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e621af",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_predictions(my_words, freq_grams, normlize=1, vocabulary=vocabulary):\n",
    "    \"\"\"\n",
    "    Generate predictions for the conditional probability of the next word given a sequence.\n",
    "\n",
    "    Args:\n",
    "        my_words (list): A list of words in the input sequence.\n",
    "        freq_grams (dict): A dictionary containing frequency of n-grams.\n",
    "        normlize (int): A normalization factor for calculating probabilities.\n",
    "        vocabulary (list): A list of words in the vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of predicted words along with their probabilities, sorted in descending order.\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_probabilities = {}  # Initialize a dictionary to store predicted word probabilities\n",
    "\n",
    "    context_size = len(list(freq_grams.keys())[0])  # Determine the context size from n-grams keys\n",
    "\n",
    "    # Preprocess input words and take only the relevant context words\n",
    "    my_tokens = preprocess(my_words)[0:context_size - 1]\n",
    "\n",
    "    # Calculate probabilities for each word in the vocabulary given the context\n",
    "    for next_word in vocabulary:\n",
    "        temp = my_tokens.copy()\n",
    "        temp.append(next_word)  # Add the next word to the context\n",
    "\n",
    "        # Calculate the conditional probability using the frequency information\n",
    "        if normlize!=0:\n",
    "            vocab_probabilities[next_word] = freq_grams[tuple(temp)] / normlize\n",
    "        else:\n",
    "            vocab_probabilities[next_word] = freq_grams[tuple(temp)] \n",
    "    # Sort the predicted words based on their probabilities in descending order\n",
    "    vocab_probabilities = sorted(vocab_probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return vocab_probabilities  # Return the sorted list of predicted words and their probabilities\n",
    "\n",
    "\n",
    "\n",
    "my_song=\"\"\n",
    "for w in tokens[0:100]:\n",
    "  my_word=make_predictions(w,freq_bigrams)[0][0]\n",
    "  my_song+=\" \"+my_word\n",
    "\n",
    "my_song=\"i\"\n",
    "\n",
    "for i in range(100):\n",
    "    my_word=make_predictions(my_word,freq_bigrams)[0][0]\n",
    "    my_song+=\" \"+my_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e954d7d2",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "#trigrams\n",
    "freq_trigrams  = nltk.FreqDist(nltk.trigrams(tokens))\n",
    "make_predictions(\"so do\",freq_trigrams,normlize=freq_bigrams[('do','i')] )[0:10]\n",
    "\n",
    "my_song=\"\"\n",
    "\n",
    "w1=tokens[0]\n",
    "for w2 in tokens[0:100]:\n",
    "    gram=w1+' '+w2\n",
    "    my_word=make_predictions(gram,freq_trigrams )[0][0]\n",
    "    my_song+=\" \"+my_word\n",
    "    w1=w2"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
